{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 线性回归的从零开始实现\n",
    ":label:`sec_linear_scratch`\n",
    "\n",
    "在了解线性回归的关键思想之后，我们可以开始通过代码来动手实现线性回归了。\n",
    "在这一节中，(**我们将从零开始实现整个方法，\n",
    "包括数据流水线、模型、损失函数和小批量随机梯度下降优化器**)。\n",
    "虽然现代的深度学习框架几乎可以自动化地进行所有这些工作，但从零开始实现可以确保你真正知道自己在做什么。\n",
    "同时，了解更细致的工作原理将方便我们自定义模型、自定义层或自定义损失函数。\n",
    "在这一节中，我们将只使用张量和自动求导。\n",
    "在之后的章节中，我们会充分利用深度学习框架的优势，介绍更简洁的实现方式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#在jupyter中，matplotlib是内嵌的，不需要import\n",
    "\n",
    "import random  #为了做random的gradient decent\n",
    "import torch\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from d2l import torch as d2l #credit to Mu Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## 生成数据集\n",
    "\n",
    "为了简单起见，我们将[**根据带有噪声的线性模型构造一个人造数据集。**]\n",
    "我们的任务是使用这个有限样本的数据集来恢复这个模型的参数。\n",
    "我们将使用低维数据，这样可以很容易地将其可视化。\n",
    "在下面的代码中，我们生成一个包含1000个样本的数据集，\n",
    "每个样本包含从标准正态分布中采样的2个特征。\n",
    "我们的合成数据集是一个矩阵$\\mathbf{X}\\in \\mathbb{R}^{1000 \\times 2}$。\n",
    "\n",
    "(**我们使用线性模型参数$\\mathbf{w} = [2, -3.4]^\\top$、$b = 4.2$\n",
    "和噪声项$\\epsilon$生成数据集及其标签：\n",
    "\n",
    "$$\\mathbf{y}= \\mathbf{X} \\mathbf{w} + b + \\mathbf\\epsilon.$$\n",
    "**)\n",
    "\n",
    "你可以将$\\epsilon$视为模型预测和标签时的潜在观测误差。\n",
    "在这里我们认为标准假设成立，即$\\epsilon$服从均值为0的正态分布。\n",
    "为了简化问题，我们将标准差设为0.01。\n",
    "下面的代码生成合成数据集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nicole\n",
    "def stringToList(string):\n",
    "    listRes = list(string.split(\",\"))\n",
    "    return listRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nicole\n",
    "def plaintext_to_value(text):\n",
    "    dic = {'email':1,\n",
    "    'banking': 2,\n",
    "    'healthcare': 3,\n",
    "    'door locker':4,\n",
    "    'camera': 5,\n",
    "    'call assistant': 6,\n",
    "    'video call': 7,\n",
    "    'location': 8,\n",
    "    'voice recording': 9,\n",
    "    'todo': 10,\n",
    "    'sleep hours': 11,\n",
    "    'playlists': 12,\n",
    "    'thermostat': 13,\n",
    "    'shopping': 14,\n",
    "    'weather': 15,\n",
    "    'your parents':1, \n",
    "    'your partner':2, \n",
    "    'your siblings':3, \n",
    "    'your housemates':4, \n",
    "    'your children':5, \n",
    "    'neighbours':6, \n",
    "    'close friends':7,\n",
    "    'your friends':7, \n",
    "    'close family':8, \n",
    "    'house helper/keeper':9,\n",
    "    'house keeper':9,\n",
    "    'house hepler/keeper':9, \n",
    "    'house keeper/helper':9, \n",
    "    'visitors in general':10, \n",
    "    'assistant provider':11,\n",
    "    'skills':12,\n",
    "    'other skills':13,\n",
    "    'advertising agencies':14,\n",
    "    'law enforcement agencies':15,\n",
    "    'no purpose&no condition': 1,\n",
    "    'with purpose&no condition':1,\n",
    "    'with purpose&condition1':2,\n",
    "    'with purpose&condition2':3,\n",
    "    'with purpose&condition3': 4,\n",
    "    'with purpose&condition4': 5,\n",
    "    'with purpose&condition5': 6,\n",
    "    'neutral': 0,\n",
    "    'completely acceptable': 1,\n",
    "    'somewhat acceptable': 1,\n",
    "    'completely unacceptable': 0,\n",
    "    'somewhat unacceptable': 0,\n",
    "    'neutral acceptable': 1\n",
    "    }\n",
    "    return dic[text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_data(plain_filename):\n",
    "    '''\n",
    "    This function is used to process the original data file,\n",
    "    and split the dataset to a training set and a test set,\n",
    "    then the data could be used for k-fold validation.\n",
    "\n",
    "    Input: plaintext-file\n",
    "    Output: numerical-list\n",
    "    '''\n",
    "\n",
    "    f = open(plain_filename)\n",
    "    reader = csv.reader(f)\n",
    "    list_data = []\n",
    "    for index, row in enumerate(reader):\n",
    "        line_list = []\n",
    "        for num, item in enumerate(row):\n",
    "            buffer = []\n",
    "            if len(item) != 0 :\n",
    "                item = item.strip(\"][\")\n",
    "                new_item = stringToList(item)\n",
    "                buffer.append(plaintext_to_value(new_item[0].strip(\"' '\")))\n",
    "                buffer.append(plaintext_to_value(new_item[1].strip(\"' '\")))\n",
    "                buffer.append(plaintext_to_value(new_item[2].strip(\"' '\")))\n",
    "                buffer.append(plaintext_to_value(new_item[-1].strip(\"' '\")))\n",
    "                line_list.append(buffer)\n",
    "        list_data.append(line_list)\n",
    "    \n",
    "\n",
    "    return list_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data_list, i):\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    for index, rows in enumerate(data_list):\n",
    "        if index == i:\n",
    "            test_list.append(rows)\n",
    "        else:\n",
    "            train_list.append(rows)\n",
    "\n",
    "    test_array = np.array(test_list[0])\n",
    "    label_test = []\n",
    "    for x in test_list[0]:\n",
    "        label_test.append(x[-1])\n",
    "    test_len = len(label_test)\n",
    "    # label_test.append(x[-1] for x in test_list[0])\n",
    "    df_1 = pd.DataFrame(test_array, columns = ['datatype', 'recipient', 'condition', 'class'])\n",
    "    test_df = pd.DataFrame(df_1, columns = ['datatype', 'recipient', 'condition'])\n",
    "    test_label_df = pd.DataFrame(df_1, columns = ['class'])\n",
    "    X_test = test_df.to_numpy()\n",
    "    y_test = np.array(label_test)\n",
    "\n",
    "\n",
    "    train_data_in_list = []\n",
    "    for num, row in enumerate(train_list):\n",
    "        for index, value in enumerate(row):\n",
    "            train_data_in_list.append(value)\n",
    "\n",
    "    label_train = []\n",
    "    for x in train_data_in_list:\n",
    "        label_train.append(x[-1])\n",
    "    df_2 = pd.DataFrame(train_data_in_list, columns = ['datatype', 'recipient', 'condition', 'class'])\n",
    "    train_df = pd.DataFrame(df_2, columns = ['datatype', 'recipient', 'condition'])\n",
    "    train_label_df = pd.DataFrame(df_2, columns = ['class'])\n",
    "    X_train = train_df.to_numpy()\n",
    "    y_train = np.array(label_train)\n",
    "    # import pdb; pdb.set_trace()\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, test_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255926,\n",
       " array([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nicole\n",
    "\n",
    "data = \"../Data/final_data.csv\"  #find data file\n",
    "data_list = trans_data(data)\n",
    "random.seed(1)\n",
    "random.shuffle(data_list)\n",
    "# print(data_list[:10])\n",
    "i = 1\n",
    "X_train, X_test, y_train, y_test, test_len = train_test_split(data_list, i)\n",
    "len(y_train), y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "origin_pos": 5,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"生成y=Xw+b+噪声\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape) \n",
    "    return X, y.reshape((-1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:**\n",
    "\n",
    "`torch.normal(mean, std, *, generator = None, out = None)` --> Tensor\n",
    "X 是一个均值为0，方差为1的随机样本，大小是 `(num_examples,len(w))`\n",
    "`y += torch.normal(0, 0.01, y.shape)` 加入一个噪声，长度和y一样\n",
    "`return X, y.reshape((-1, 1))` -1表示未指定，行数由pytorch推断，reshape是要把y展开成一行一列，不然y是一个标量\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [11],\n",
       "       [12]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# About `reshape`\n",
    "import numpy as np\n",
    "z = np.array([[1, 2, 3, 4],\n",
    "              [5, 6, 7, 8],\n",
    "              [9, 10, 11, 12]])\n",
    "print(z.shape)\n",
    "#Output = (3,4)\n",
    "\n",
    "'''\n",
    "和z.reshape((-1,1))结果相同，It means unknown dimension and we want numpy to figure it out.\n",
    "Numpy will figure this by looking at the \"length of the array and remaining dimensions\" and making sure it satisfies the above mentioned criteria.\n",
    "'''\n",
    "z.reshape(-1,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 1., 1.]) tensor([1.])\n",
      "torch.float32 torch.float32\n"
     ]
    }
   ],
   "source": [
    "#nicole \n",
    "# features = torch.tensor(X_train)\n",
    "\n",
    "# y_tensor = torch.tensor(y_train, dtype=torch.long, device=device)\n",
    "\n",
    "features = torch.tensor(X_train, dtype=torch.float32)\n",
    "labels = torch.tensor(y_train.reshape(-1,1),dtype = torch.float32)\n",
    "\n",
    "\n",
    "print(features[0], labels[0])\n",
    "print(features.dtype, labels.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "注意，[**`features`中的每一行都包含一个二维数据样本，\n",
    "`labels`中的每一行都包含一维标签值（一个标量）**]。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: tensor([3., 1., 1.]) \n",
      "label: tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "print('features:', features[0],'\\nlabel:', labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "通过生成第二个特征`features[:, 1]`和`labels`的散点图，\n",
    "可以直观观察到两者之间的线性关系。\n",
    "\n",
    "**这是我们自己提供了w和b生成的数据集，本身内部含有线性关系，是为了用于之后的model training！！**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#d2l.set_figsize()\n",
    "#d2l.plt.scatter(features[:, (1)].detach().numpy(), labels.detach().numpy(), 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "## 读取数据集\n",
    "\n",
    "回想一下，训练模型时要对数据集进行遍历，每次抽取一小批量样本，并使用它们来更新我们的模型。\n",
    "由于这个过程是训练机器学习算法的基础，所以有必要定义一个函数，\n",
    "该函数能<font color=red>**打乱**数据集中的样本并以**小批量**方式获取数据</font>。\n",
    "\n",
    "在下面的代码中，我们[**定义一个`data_iter`函数，\n",
    "该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为`batch_size`的小批量**]。\n",
    "每个小批量包含一组特征和标签。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))   #产生一个列表，列表存放了index of features\n",
    "    # 这些样本是随机读取的，没有特定的顺序\n",
    "    random.shuffle(indices)   #然后将index shuffle掉，读取时，就是random的一个顺序了\n",
    "    for i in range(0, num_examples, batch_size):  \n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "**Comments**\n",
    "1. `for i in range(0, num_examples, batch_size): ` 从0开始，到num_examples，每次挑batch_size个sample\n",
    "2. `indices[i: min(i + batch_size, num_examples)]` 这里取值开头是i，然后到 i+batch_size与num_examples的最小值，因为最后一个batch，可能包含的数据不到batch_size那么多，所以这个时候应该取i到num_examples，避免出错\n",
    "3. 最后取出的是某个batch_size的features和labels\n",
    "\n",
    "通常，我们利用GPU并行运算的优势，处理合理大小的“小批量”。\n",
    "每个样本都可以并行地进行模型计算，且每个样本损失函数的梯度也可以被并行计算。\n",
    "GPU可以在处理几百个样本时，所花费的时间不比处理一个样本时多太多。\n",
    "\n",
    "我们直观感受一下小批量运算：读取第一个小批量数据样本并打印。\n",
    "每个批量的特征维度显示批量大小和输入特征数。\n",
    "同样的，批量的标签形状与`batch_size`相等。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9., 12.,  2.],\n",
      "        [ 4., 15.,  4.],\n",
      "        [10., 12.,  5.],\n",
      "        [ 9., 12.,  1.],\n",
      "        [15.,  6.,  1.]]) \n",
      " tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "#print(data_iter(batch_size, features, labels))   会得到一个object，无法print出来所有的数据\n",
    "\n",
    "for X, y in data_iter(batch_size, features, labels):\n",
    "    print(X, '\\n', y)\n",
    "    break  #只print一个batch中的数据，然后就break 不然会print出所有的batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "当我们运行迭代时，我们会连续地获得不同的小批量，直至遍历完整个数据集。\n",
    "上面实现的迭代对于教学来说很好，但它的执行效率很低，可能会在实际问题上陷入麻烦。\n",
    "例如，它要求我们将所有数据加载到内存中，并执行大量的随机内存访问。\n",
    "<font color=red>在深度学习框架中实现的内置迭代器效率要高得多，它可以处理存储在文件中的数据和数据流提供的数据。</font>\n",
    "\n",
    "\n",
    "## 初始化模型参数\n",
    "\n",
    "[**在我们开始用小批量随机梯度下降优化我们的模型参数之前**]，\n",
    "(**我们需要先有一些参数**)。\n",
    "在下面的代码中，我们通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重，\n",
    "并将偏置初始化为0。\n",
    "\n",
    "`requires_grad`都要设置为True，因为w和b都需要被更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.normal(0, 0.01, size=(3,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "w,b\n",
    "w.dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 21
   },
   "source": [
    "在初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据。\n",
    "每次更新都需要计算损失函数关于模型参数的梯度。\n",
    "**有了这个梯度，我们就可以向减小损失的方向更新每个参数。**\n",
    "因为手动计算梯度很枯燥而且容易出错，所以没有人会手动计算梯度。\n",
    "我们使用 :numref:`sec_autograd`中引入的自动微分来计算梯度。\n",
    "\n",
    "## 定义模型\n",
    "\n",
    "接下来，我们必须[**定义模型，将模型的输入和参数同模型的输出关联起来。**]\n",
    "回想一下，要计算线性模型的输出，\n",
    "我们只需计算输入特征$\\mathbf{X}$和模型权重$\\mathbf{w}$的矩阵-向量乘法后加上偏置$b$。\n",
    "<font color=red>注意，上面的$\\mathbf{Xw}$是一个向量，而$b$是一个标量。</font>\n",
    "回想一下 :numref:`subsec_broadcasting`中描述的广播机制：\n",
    "当我们用一个向量加一个标量时，标量会被加到向量的每个分量上。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def linreg(X, w, b):  #@save\n",
    "    \"\"\"线性回归模型\"\"\"\n",
    "    return torch.matmul(X, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 23
   },
   "source": [
    "## [**定义损失函数**]\n",
    "\n",
    "因为需要计算损失函数的梯度，所以我们应该先定义损失函数。\n",
    "这里我们使用 :numref:`sec_linear_regression`中描述的平方损失函数。\n",
    "在实现中，我们需要将真实值`y`的形状转换为和预测值`y_hat`的形状相同。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"均方损失\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2   #这里 /2 是为了求导方便"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 25
   },
   "source": [
    "## (**定义优化算法**)\n",
    "\n",
    "正如我们在 :numref:`sec_linear_regression`中讨论的，线性回归有解析解。\n",
    "尽管线性回归有解析解，但本书中的其他模型却没有。\n",
    "这里我们介绍小批量随机梯度下降。\n",
    "\n",
    "在每一步中，使用从数据集中随机抽取的一个小批量，然后根据参数计算损失的梯度。\n",
    "接下来，朝着减少损失的方向更新我们的参数。\n",
    "下面的函数实现小批量随机梯度下降更新。\n",
    "该函数接受模型参数集合、学习速率和批量大小作为输入。每\n",
    "一步更新的大小由学习速率`lr`决定。\n",
    "因为我们计算的损失是一个批量样本的总和，所以我们用批量大小（`batch_size`）\n",
    "来规范化步长，这样步长大小就不会取决于我们对批量大小的选择。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):  #@save\n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    with torch.no_grad():  \n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**\n",
    "1. params是一个list，包含了所有的参数\n",
    "2. `with torch.no_grad():` 更新的时候不需要计算梯度，且这里只是函数定义，并不需要更新梯度\n",
    "3. 计算的损失是一个批量样本的总和，所以用批量大小batch_size来归一化步长，这样步长大小就不会取决于我么对批量大小的选择\n",
    "4. `param -= lr * param.grad / batch_size` 每一个parameter，可能是w或b，都要减去在负梯度上面乘以learning rate再除以batch size\n",
    "5. `param.grad.zero()` 要在下一次计算梯度之前手动清零梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 29
   },
   "source": [
    "## 训练\n",
    "\n",
    "现在我们已经准备好了模型训练所有需要的要素，可以实现主要的[**训练过程**]部分了。\n",
    "理解这段代码至关重要，因为从事深度学习后，\n",
    "你会一遍又一遍地看到几乎相同的训练过程。\n",
    "在每次迭代中，我们读取一小批量训练样本，并通过我们的模型来获得一组预测。\n",
    "计算完损失后，我们开始反向传播，存储每个参数的梯度。\n",
    "最后，我们调用优化算法`sgd`来更新模型参数。\n",
    "\n",
    "<font color=red>**概括一下，我们将执行以下循环：**</font>\n",
    "\n",
    "* 初始化参数\n",
    "* 重复以下训练，直到完成\n",
    "    * 计算梯度$\\mathbf{g} \\leftarrow \\partial_{(\\mathbf{w},b)} \\frac{1}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} l(\\mathbf{x}^{(i)}, y^{(i)}, \\mathbf{w}, b)$\n",
    "    * 更新参数$(\\mathbf{w}, b) \\leftarrow (\\mathbf{w}, b) - \\eta \\mathbf{g}$\n",
    "\n",
    "在每个*迭代周期*（epoch）中，我们使用`data_iter`函数遍历整个数据集，\n",
    "并将训练数据集中所有样本都使用一次（假设样本数能够被批量大小整除）。\n",
    "这里的迭代周期个数`num_epochs`和学习率`lr`都是超参数，分别设为3和0.03。\n",
    "设置超参数很棘手，需要通过反复试验进行调整。\n",
    "我们现在忽略这些细节，以后会在 :numref:`chap_optimization`中详细介绍。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "num_epochs = 3\n",
    "net = linreg\n",
    "loss = squared_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "origin_pos": 32,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.074228\n",
      "epoch 2, loss 0.270222\n",
      "epoch 3, loss 0.189850\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels): \n",
    "        l = loss(net(X, w, b), y)  # X和y的小批量损失\n",
    "        # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，\n",
    "        # 并以此计算关于[w,b]的梯度\n",
    "        l.sum().backward()\n",
    "        sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features, w, b), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**\n",
    "1. `for X, y in data_iter(batch_size, features, labels):` 每次拿出一整个batch的X和y\n",
    "2. `l = loss(net(X, w, b), y)` 把预测出来的y hat和y进行比较，计算出loss  **l就是一个长为batch_size的向量**\n",
    "3. `l.sum().backward()` 求和后再算梯度\n",
    "4. 每次修改超参数or每次重新run 都要重新从更新w,b开始"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 34
   },
   "source": [
    "因为我们使用的是自己合成的数据集，所以我们知道真正的参数是什么。\n",
    "因此，我们可以通过[**比较真实参数和通过训练学到的参数来评估训练的成功程度**]。\n",
    "事实上，真实参数和通过训练学到的参数确实非常接近。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "origin_pos": 35,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的估计误差: tensor([7.5579e-05, 2.0051e-04], grad_fn=<SubBackward0>)\n",
      "b的估计误差: tensor([-0.0009], grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')\n",
    "print(f'b的估计误差: {true_b - b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 36
   },
   "source": [
    "注意，我们不应该想当然地认为我们能够完美地求解参数。\n",
    "在机器学习中，我们通常不太关心恢复真正的参数，而更关心如何高度准确预测参数。\n",
    "幸运的是，即使是在复杂的优化问题上，随机梯度下降通常也能找到非常好的解。\n",
    "其中一个原因是，在深度网络中存在许多参数组合能够实现高度精确的预测。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 我们学习了深度网络是如何实现和优化的。在这一过程中只使用张量和自动微分，不需要定义层或复杂的优化器。\n",
    "* 这一节只触及到了表面知识。在下面的部分中，我们将基于刚刚介绍的概念描述其他模型，并学习如何更简洁地实现其他模型。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 如果我们将权重初始化为零，会发生什么。算法仍然有效吗？\n",
    "1. 假设你是[乔治·西蒙·欧姆](https://en.wikipedia.org/wiki/Georg_Ohm)，试图为电压和电流的关系建立一个模型。你能使用自动微分来学习模型的参数吗?\n",
    "1. 您能基于[普朗克定律](https://en.wikipedia.org/wiki/Planck%27s_law)使用光谱能量密度来确定物体的温度吗？\n",
    "1. 如果你想计算二阶导数可能会遇到什么问题？你会如何解决这些问题？\n",
    "1. 为什么在`squared_loss`函数中需要使用`reshape`函数？\n",
    "1. 尝试使用不同的学习率，观察损失函数值下降的快慢。\n",
    "1. 如果样本个数不能被批量大小整除，`data_iter`函数的行为会有什么变化？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 38,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1778)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
